{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253705df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required lib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from utils import pct_change, preprocess_and_split_df, convert_to_dataset, data_preparation, plot_data, create_bollinger_limits\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import random\n",
    "import torch\n",
    "import statistics\n",
    "from LSTM_model_future_change import LSTM_FutureChangeGeneral, LSTM_FutureChange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0898860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data dict again \n",
    "CRYPTO_DICT = { \n",
    "            \"ADA\": {}, \n",
    "            \"BCH\": {},\n",
    "            \"BNB\": {}, \n",
    "            \"BTC\": {},\n",
    "            \"ETH\": {},\n",
    "            \"LTC\": {}, \n",
    "            \"NEO\": {},\n",
    "            \"TRX\": {},\n",
    "            \"XRP\": {},\n",
    "}\n",
    "\n",
    "# upload the data \n",
    "\n",
    "for Crypto_code in CRYPTO_DICT:\n",
    "    CRYPTO_DICT[Crypto_code][\"df\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-main_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_train\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-train_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_val\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-val_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_test\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1f3197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unix', 'date', 'symbol', 'close', 'mid_price_1', 'mid_price_2',\n",
       "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
       "       'upper_check', 'lower_check', 'future_price', 'future_change'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRYPTO_DICT[\"BTC\"][\"df_train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a49cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for future price prediction \n",
    "future_price_columns =  ['close', 'mid_price_1', 'mid_price_2',\n",
    "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
    "       'upper_check', 'lower_check', 'future_price']\n",
    "\n",
    "future_price_length = (len(future_price_columns) - 1)\n",
    "\n",
    "# Dataset for future change prediction \n",
    "future_change_columns = ['close', 'mid_price_1', 'mid_price_2',\n",
    "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
    "       'upper_check', 'lower_check','future_change']\n",
    "future_change_length = (len(future_change_columns) -  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd2dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_price_dataset start converting for all coins\n",
      "future_price_dataset finished converting for all coins\n"
     ]
    }
   ],
   "source": [
    "datasets = [[\"future_price_dataset\", future_price_columns, future_price_length]]\n",
    "for dataset in datasets: \n",
    "    print(f\"{datasets[0][0]} start converting for all coins\")\n",
    "    for Crypto_code in CRYPTO_DICT:\n",
    "        if \"datasets\" not in list(CRYPTO_DICT[Crypto_code].keys()):\n",
    "            CRYPTO_DICT[Crypto_code][\"datasets\"] = {}\n",
    "        for df in CRYPTO_DICT[Crypto_code]: \n",
    "            if df == \"df\" or df == \"datasets\": \n",
    "                pass\n",
    "            else:\n",
    "                if dataset[0] not in list(CRYPTO_DICT[Crypto_code][\"datasets\"].keys()):\n",
    "                    CRYPTO_DICT[Crypto_code][\"datasets\"][dataset[0]] = {}\n",
    "                CRYPTO_DICT[Crypto_code][\"datasets\"][dataset[0]][f\"{df}_dataset\"] = convert_to_dataset(CRYPTO_DICT[Crypto_code][df], dataset[1], dataset[2])\n",
    "    print(f\"{datasets[0][0]} finished converting for all coins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b47e9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix</th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>close</th>\n",
       "      <th>mid_price_1</th>\n",
       "      <th>mid_price_2</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA0</th>\n",
       "      <th>EMA1</th>\n",
       "      <th>EMA2</th>\n",
       "      <th>RSI</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper_check</th>\n",
       "      <th>lower_check</th>\n",
       "      <th>future_price</th>\n",
       "      <th>future_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1605534120</td>\n",
       "      <td>11/16/2020</td>\n",
       "      <td>ADA/USDT</td>\n",
       "      <td>0.569916</td>\n",
       "      <td>0.104354</td>\n",
       "      <td>0.573263</td>\n",
       "      <td>-0.503231</td>\n",
       "      <td>0.874844</td>\n",
       "      <td>0.173305</td>\n",
       "      <td>1.473872</td>\n",
       "      <td>0.278490</td>\n",
       "      <td>98.371336</td>\n",
       "      <td>1.328975</td>\n",
       "      <td>-0.188765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.523333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1605534180</td>\n",
       "      <td>11/16/2020</td>\n",
       "      <td>ADA/USDT</td>\n",
       "      <td>0.569443</td>\n",
       "      <td>0.466574</td>\n",
       "      <td>0.582190</td>\n",
       "      <td>-0.608049</td>\n",
       "      <td>1.030657</td>\n",
       "      <td>-0.351334</td>\n",
       "      <td>0.772922</td>\n",
       "      <td>0.720969</td>\n",
       "      <td>98.507463</td>\n",
       "      <td>1.571223</td>\n",
       "      <td>-0.228248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.911994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1605534240</td>\n",
       "      <td>11/16/2020</td>\n",
       "      <td>ADA/USDT</td>\n",
       "      <td>-0.281825</td>\n",
       "      <td>0.462010</td>\n",
       "      <td>0.359231</td>\n",
       "      <td>-0.585942</td>\n",
       "      <td>1.154065</td>\n",
       "      <td>0.584498</td>\n",
       "      <td>-0.366869</td>\n",
       "      <td>0.457446</td>\n",
       "      <td>92.559524</td>\n",
       "      <td>1.001169</td>\n",
       "      <td>0.474485</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.504723</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1605534300</td>\n",
       "      <td>11/16/2020</td>\n",
       "      <td>ADA/USDT</td>\n",
       "      <td>-0.501578</td>\n",
       "      <td>-0.421320</td>\n",
       "      <td>-0.577299</td>\n",
       "      <td>-0.595586</td>\n",
       "      <td>0.922450</td>\n",
       "      <td>1.547995</td>\n",
       "      <td>0.108910</td>\n",
       "      <td>0.648270</td>\n",
       "      <td>83.557951</td>\n",
       "      <td>0.607406</td>\n",
       "      <td>0.565131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.406365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1605534360</td>\n",
       "      <td>11/16/2020</td>\n",
       "      <td>ADA/USDT</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>0.048246</td>\n",
       "      <td>0.247989</td>\n",
       "      <td>-0.590355</td>\n",
       "      <td>1.429333</td>\n",
       "      <td>0.432349</td>\n",
       "      <td>0.662095</td>\n",
       "      <td>0.192357</td>\n",
       "      <td>83.172414</td>\n",
       "      <td>0.516682</td>\n",
       "      <td>1.284834</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.339432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0         unix        date    symbol     close  mid_price_1  mid_price_2  \\\n",
       "10  1605534120  11/16/2020  ADA/USDT  0.569916     0.104354     0.573263   \n",
       "11  1605534180  11/16/2020  ADA/USDT  0.569443     0.466574     0.582190   \n",
       "12  1605534240  11/16/2020  ADA/USDT -0.281825     0.462010     0.359231   \n",
       "13  1605534300  11/16/2020  ADA/USDT -0.501578    -0.421320    -0.577299   \n",
       "14  1605534360  11/16/2020  ADA/USDT  0.858088     0.048246     0.247989   \n",
       "\n",
       "0  Volume USDT       SMA      EMA0      EMA1      EMA2        RSI     upper  \\\n",
       "10   -0.503231  0.874844  0.173305  1.473872  0.278490  98.371336  1.328975   \n",
       "11   -0.608049  1.030657 -0.351334  0.772922  0.720969  98.507463  1.571223   \n",
       "12   -0.585942  1.154065  0.584498 -0.366869  0.457446  92.559524  1.001169   \n",
       "13   -0.595586  0.922450  1.547995  0.108910  0.648270  83.557951  0.607406   \n",
       "14   -0.590355  1.429333  0.432349  0.662095  0.192357  83.172414  0.516682   \n",
       "\n",
       "0      lower  upper_check  lower_check  future_price  future_change  \n",
       "10 -0.188765            1            0     -0.523333              0  \n",
       "11 -0.228248            1            0      0.911994              1  \n",
       "12  0.474485            1            0      1.504723              1  \n",
       "13  0.565131            1            0     -0.406365              1  \n",
       "14  1.284834            1            0     -1.339432              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRYPTO_DICT[\"ADA\"][\"df_train\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42990597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 list for all Crypto_coin data\n",
    "train_dataset_all = []\n",
    "val_dataset_all = []\n",
    "test_dataset_all = []\n",
    "\n",
    "for Crypto_code in CRYPTO_DICT:\n",
    "  train_dataset_all += CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "  del CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "  val_dataset_all += CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "  del CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "  test_dataset_all += CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "  del CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d70f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "del CRYPTO_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979cb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset: 1393341\n",
      "len of val dataset: 173696\n",
      "len of test dataset: 173689\n"
     ]
    }
   ],
   "source": [
    "print(f\"len of train dataset: {len(train_dataset_all)}\")\n",
    "print(f\"len of val dataset: {len(val_dataset_all)}\")\n",
    "print(f\"len of test dataset: {len(test_dataset_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12fff514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6992e-01,  1.0435e-01,  5.7326e-01, -5.0323e-01,  8.7484e-01,\n",
       "          1.7331e-01,  1.4739e+00,  2.7849e-01,  9.8371e+01,  1.3290e+00,\n",
       "         -1.8876e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 5.6944e-01,  4.6657e-01,  5.8219e-01, -6.0805e-01,  1.0307e+00,\n",
       "         -3.5133e-01,  7.7292e-01,  7.2097e-01,  9.8507e+01,  1.5712e+00,\n",
       "         -2.2825e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-2.8182e-01,  4.6201e-01,  3.5923e-01, -5.8594e-01,  1.1541e+00,\n",
       "          5.8450e-01, -3.6687e-01,  4.5745e-01,  9.2560e+01,  1.0012e+00,\n",
       "          4.7448e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-5.0158e-01, -4.2132e-01, -5.7730e-01, -5.9559e-01,  9.2245e-01,\n",
       "          1.5480e+00,  1.0891e-01,  6.4827e-01,  8.3558e+01,  6.0741e-01,\n",
       "          5.6513e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 8.5809e-01,  4.8246e-02,  2.4799e-01, -5.9036e-01,  1.4293e+00,\n",
       "          4.3235e-01,  6.6209e-01,  1.9236e-01,  8.3172e+01,  5.1668e-01,\n",
       "          1.2848e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.4196e+00,  1.9703e+00,  1.5927e+00, -4.9004e-01,  1.7207e+00,\n",
       "         -9.0356e-01,  3.3282e-01, -1.2573e-01,  8.4882e+01,  1.4264e+00,\n",
       "          7.7141e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-3.9078e-01,  7.3917e-02,  7.6135e-01, -5.8605e-01,  1.4728e+00,\n",
       "         -1.1607e-03,  5.6880e-01, -8.4134e-03,  7.9930e+01,  9.4699e-01,\n",
       "          9.2424e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.2747e+00, -1.0654e+00, -1.2283e+00, -5.7961e-01,  1.0910e+00,\n",
       "          7.1025e-01,  2.3422e-02,  2.3754e-01,  6.5541e+01, -4.9467e-02,\n",
       "          1.4084e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 5.0732e-01, -3.8228e-01, -6.5091e-01, -6.0264e-01,  1.1568e+00,\n",
       "          1.8344e-01, -3.1147e-01, -3.0080e-01,  6.6065e+01,  6.4933e-02,\n",
       "          1.3798e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 8.0181e-01,  7.3292e-01,  9.8470e-01, -6.0484e-01,  1.3639e+00,\n",
       "          4.9400e-01, -1.1454e-01,  1.4303e-01,  6.9726e+01,  2.3628e-01,\n",
       "          1.4726e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.9916e-01,  9.0011e-01,  4.2346e-01, -6.0020e-01,  1.0576e+00,\n",
       "         -2.2030e-01,  2.0465e-01,  3.8403e-01,  6.7684e+01,  3.3035e-01,\n",
       "          1.0001e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 4.5168e-01, -5.5368e-01,  1.9650e-01, -6.0113e-01,  9.5187e-01,\n",
       "         -5.1103e-01, -4.2993e-01,  8.8931e-01,  6.8037e+01,  7.7542e-01,\n",
       "          4.4015e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-5.3460e-01,  2.7179e-01, -3.0255e-02, -5.7987e-01,  8.2474e-01,\n",
       "         -1.2507e-01,  1.4295e-01,  7.1882e-01,  6.3005e+01,  2.5304e-01,\n",
       "          7.8418e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-4.5962e-01, -7.5593e-01, -6.8265e-01, -5.8820e-01,  6.9591e-01,\n",
       "          2.8550e-01,  4.1832e-01,  1.3212e+00,  5.9253e+01, -9.3910e-02,\n",
       "          9.5723e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.4347e-01, -1.5409e-01, -2.3863e-01, -5.8059e-01,  6.2968e-01,\n",
       "         -5.6024e-01,  9.8166e-01,  5.8730e-01,  5.7190e+01, -5.1855e-03,\n",
       "          7.8920e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 3.9022e-01,  6.0339e-01,  3.8191e-01, -5.8278e-01,  7.4027e-01,\n",
       "          2.6727e-01,  7.0963e-01,  1.2196e+00,  5.6260e+01,  4.9879e-03,\n",
       "          9.1698e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-8.0212e-01, -3.1746e-01, -2.8489e-01, -5.8801e-01,  5.0622e-01,\n",
       "          5.1789e-01,  1.3821e+00,  4.6015e-01,  5.2904e+01, -3.8838e-01,\n",
       "          1.0032e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 6.1679e-01, -3.0896e-01, -1.7383e-01, -5.8216e-01,  5.7219e-01,\n",
       "          1.0898e+00,  4.3612e-01,  7.3387e-01,  5.9165e+01, -1.6291e-01,\n",
       "          8.6841e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 4.3121e-01,  6.3783e-01,  6.5983e-01, -5.9196e-01,  6.3099e-01,\n",
       "          5.5414e-01,  1.1909e+00,  6.7458e-01,  5.7108e+01, -6.5032e-03,\n",
       "          7.9128e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 9.3774e-01,  8.6960e-01,  1.0296e+00, -5.8358e-01,  8.4500e-01,\n",
       "          1.3420e+00,  2.4588e-01,  4.6713e-01,  5.4522e+01,  5.6714e-01,\n",
       "          5.0729e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 6.3043e-03,  7.3124e-01,  6.4939e-01, -5.1771e-01,  6.9850e-01,\n",
       "         -7.3915e-04,  6.0455e-01,  5.0758e-01,  5.7362e+01,  6.5310e-01,\n",
       "          2.4260e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.2032e+00,  7.0487e-01,  8.3374e-01, -6.0714e-01,  8.6423e-01,\n",
       "          1.0696e+00,  5.3436e-01, -1.5042e-01,  7.3885e+01,  1.8015e+00,\n",
       "         -6.5261e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-7.6524e-01,  4.8123e-01,  2.8830e-01, -5.9938e-01,  7.3740e-01,\n",
       "         -2.0233e-01,  2.9652e-01,  5.1473e-02,  6.4934e+01,  7.1195e-01,\n",
       "          2.3434e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.2092e+00, -1.7069e-01,  3.0206e-01, -5.7550e-01,  1.1830e+00,\n",
       "          4.2019e-01,  3.6873e-01,  1.0970e+00,  6.6695e+01,  1.3726e+00,\n",
       "          1.5457e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-8.3980e-01,  5.6253e-01,  2.3737e-01, -5.9830e-01,  7.4054e-01,\n",
       "          3.3405e-01, -3.8955e-01,  1.6379e+00,  6.1831e+01,  6.8665e-01,\n",
       "          2.6243e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 5.8694e-01, -3.7631e-01, -1.2703e-01, -5.8929e-01,  5.2477e-01,\n",
       "          6.2358e-02, -7.6892e-02,  5.5179e-01,  6.2414e+01,  1.0711e+00,\n",
       "         -3.7562e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.3596e-01,  7.3388e-01,  5.0487e-01, -5.8491e-01,  6.6187e-01,\n",
       "          2.2133e-01,  1.1837e+00,  5.1200e-01,  6.6879e+01,  1.0804e+00,\n",
       "         -2.1427e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.2338e-01,  3.0771e-02, -5.7832e-02, -5.8855e-01,  9.6099e-01,\n",
       "         -6.8563e-01,  1.7372e+00,  9.9280e-01,  6.9601e+01,  6.1922e-01,\n",
       "          6.0174e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 2.1103e-01,  3.9336e-02,  1.0813e-01, -5.9915e-01,  8.8414e-01,\n",
       "         -9.8615e-02,  3.5102e-01,  6.8390e-01,  6.9852e+01,  7.4580e-01,\n",
       "          3.8424e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-9.0802e-01, -5.2603e-01, -5.8333e-01, -5.9495e-01,  4.3882e-01,\n",
       "          1.4590e+00,  3.3341e-01,  1.2222e+00,  6.1335e+01,  2.9841e-01,\n",
       "          2.5972e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 2.7273e-01, -4.6205e-01, -3.1155e-01, -5.7182e-01,  5.6159e-01,\n",
       "          1.8143e+00,  9.2216e-01,  2.2941e+00,  6.8508e+01,  3.3705e-01,\n",
       "          3.7552e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.7067e+00,  1.2657e+00,  1.3217e+00, -2.6165e-01,  8.8898e-01,\n",
       "         -1.7697e-01,  5.2918e-01,  2.0799e+00,  7.2178e+01,  1.3881e+00,\n",
       "         -2.2754e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.2257e+00,  1.8290e+00,  1.9695e+00, -4.8474e-01,  1.3478e+00,\n",
       "          1.1212e-02,  1.1697e+00,  2.1180e+00,  7.4362e+01,  2.1231e+00,\n",
       "         -3.6356e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.2315e+00,  1.0758e-01,  6.1848e-02, -4.9420e-01,  1.1445e+00,\n",
       "          8.3771e-01,  2.3793e+00,  1.7464e+00,  6.3316e+01,  6.6443e-01,\n",
       "          7.8711e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.0844e-01, -4.4781e-01, -8.2550e-01, -5.8072e-01,  1.1353e+00,\n",
       "          2.6024e-01,  1.9862e+00,  2.1142e+00,  6.3672e+01,  5.6349e-01,\n",
       "          8.7278e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 9.3940e-01,  2.9997e-01,  7.2453e-01, -5.7201e-01,  1.2789e+00,\n",
       "          1.0934e+00,  1.9512e+00,  1.8518e+00,  6.2757e+01,  1.1240e+00,\n",
       "          5.1194e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.8423e-01,  6.1177e-01,  4.9407e-01, -5.8335e-01,  1.4383e+00,\n",
       "          2.4255e+00,  1.4549e+00,  2.3640e+00,  6.6503e+01,  2.8907e-01,\n",
       "          1.5151e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.0881e+00,  5.1740e-01,  6.3627e-01, -5.7309e-01,  1.5621e+00,\n",
       "          1.5609e+00,  1.8801e+00,  2.7721e+00,  6.6098e+01,  1.1081e+00,\n",
       "          8.8035e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 2.1257e+00,  2.2155e+00,  2.1835e+00, -1.7430e-01,  2.0052e+00,\n",
       "          1.4282e+00,  1.5314e+00,  1.1317e+00,  7.7799e+01,  3.0060e+00,\n",
       "         -3.9480e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 3.9222e-01,  1.2479e+00,  1.7295e+00, -5.0969e-01,  1.8639e+00,\n",
       "          7.8850e-01,  2.1287e+00,  1.5199e+00,  7.7403e+01,  2.9877e+00,\n",
       "         -5.5571e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 7.3045e-01,  7.4031e-01,  6.8807e-01, -5.8476e-01,  2.0527e+00,\n",
       "          1.4172e+00,  2.5532e+00,  7.7771e-01,  7.8602e+01,  3.0450e+00,\n",
       "         -3.7784e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 8.0530e-02,  4.5513e-01,  4.9561e-01, -3.4270e-01,  1.7608e+00,\n",
       "          9.3773e-01,  5.5173e-01,  3.3023e-01,  7.9577e+01,  2.7515e+00,\n",
       "         -4.6093e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.1555e+00,  8.5390e-01,  9.2016e-01, -5.9607e-01,  2.2613e+00,\n",
       "          1.7278e+00,  1.1146e+00, -1.2327e+00,  8.1170e+01,  3.1077e+00,\n",
       "         -1.8220e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 2.5587e-01,  5.0109e-01,  9.6946e-01, -5.3196e-01,  2.0137e+00,\n",
       "          2.1203e+00,  2.8286e-01, -1.6444e+00,  8.7897e+01,  2.9618e+00,\n",
       "         -3.5255e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.4231e+00,  1.5942e+00,  1.3605e+00, -6.0330e-01,  2.6036e+00,\n",
       "         -4.8657e-01, -1.4016e-01, -1.6585e+00,  8.9002e+01,  3.4717e+00,\n",
       "         -1.1135e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.7546e+00, -4.3426e-01, -2.0706e-01, -5.6862e-01,  2.3240e+00,\n",
       "         -3.0904e-01, -2.0986e+00,  6.8380e-01,  7.5640e+01,  2.1460e+00,\n",
       "          3.8638e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 9.8968e-01, -2.8238e-01, -5.3888e-01, -5.7906e-01,  2.6138e+00,\n",
       "         -5.7226e-01, -1.9023e+00,  4.4201e-01,  8.4795e+01,  2.3544e+00,\n",
       "          9.8396e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [-7.1374e-01,  2.5423e-01,  2.4754e-01, -6.1142e-01,  2.3702e+00,\n",
       "         -2.4257e+00, -1.7814e-01,  3.1113e-01,  8.0142e+01,  1.4953e+00,\n",
       "          1.5159e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-4.7876e-01, -8.1048e-01, -8.8001e-01, -6.2411e-01,  2.4779e+00,\n",
       "         -2.1573e+00,  9.7672e-01, -2.9299e-01,  7.5737e+01,  4.8574e-01,\n",
       "          2.6357e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-2.4333e+00, -1.7838e+00, -1.9183e+00, -6.2169e-01,  1.7696e+00,\n",
       "         -1.5715e+00,  5.7402e-01,  2.7058e-02,  6.4414e+01, -7.6953e-01,\n",
       "          2.9694e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.0757e+00, -2.5839e+00, -2.3234e+00, -6.1806e-01,  1.0484e+00,\n",
       "          6.5465e-01,  3.5995e-01,  2.0874e-02,  5.7284e+01, -5.3007e-01,\n",
       "          1.8296e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [-5.6269e-01, -6.9194e-01, -1.2001e+00, -6.2663e-01,  5.8498e-01,\n",
       "          1.6724e+00, -3.7899e-01,  1.5926e-02,  4.8224e+01, -2.8794e-01,\n",
       "          1.0127e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 1.6328e+00,  1.2723e+00,  1.3704e+00, -6.2396e-01,  1.3278e+00,\n",
       "          6.8383e-01,  3.5937e-02,  8.6946e-01,  5.2566e+01, -7.1667e-01,\n",
       "          2.3586e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [ 1.5208e+00,  1.5167e+00,  1.6335e+00, -6.2707e-01,  1.6951e+00,\n",
       "          2.7987e-01,  2.4590e-02,  9.5249e-01,  5.4980e+01, -6.3648e-01,\n",
       "          2.7382e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-1.7605e-01,  6.0691e-01,  6.5437e-01, -6.2279e-01,  1.4058e+00,\n",
       "         -6.3593e-01,  1.6486e-02,  7.9000e-01,  5.4132e+01, -6.1564e-01,\n",
       "          2.3538e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-7.4862e-02, -9.2507e-02, -9.8298e-02, -6.2715e-01,  1.4333e+00,\n",
       "          5.4560e-02,  1.0170e+00,  6.3099e-01,  5.0209e+01, -1.2187e+00,\n",
       "          2.9695e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-8.8479e-01, -1.1390e+00, -9.6913e-01, -6.2482e-01,  9.2130e-01,\n",
       "          2.4932e-02,  1.0277e+00,  5.0387e-01,  4.6429e+01, -1.1142e+00,\n",
       "          2.2256e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 4.1837e-01, -2.8991e-02, -2.8552e-01, -6.2400e-01,  4.8003e-01,\n",
       "          1.0118e-02,  7.6670e-01, -4.1222e-01,  4.2611e+01, -2.1088e-01,\n",
       "          8.0205e-01,  0.0000e+00,  1.0000e+00],\n",
       "        [ 1.3839e+00,  1.7389e+00,  1.8725e+00, -6.2720e-01,  5.2378e-01,\n",
       "          5.2895e-01, -6.7873e-01, -5.2716e-01,  4.3948e+01,  2.6607e-01,\n",
       "          3.9679e-01,  1.0000e+00,  0.0000e+00],\n",
       "        [ 4.1053e-01,  3.4340e-01,  3.7080e-01, -6.2423e-01,  3.3050e-01,\n",
       "          2.6201e-01, -2.8612e-01,  7.5418e-01,  4.8898e+01,  4.1005e-01,\n",
       "          1.7212e-02,  1.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_all[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a219",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e515eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model creation lib\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70dc81bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU.')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14e6608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c59ab39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training batches: 21770\n",
      "Number of validation batches: 2714\n",
      "Number of Testing batches: 2713\n"
     ]
    }
   ],
   "source": [
    "# num_batches\n",
    "batch_size = 64\n",
    "\n",
    "# Create Dataloaders \n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset_all, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset_all, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset_all, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(f\"Number of Training batches: {len(training_dataloader)}\")\n",
    "print(f\"Number of validation batches: {len(validation_dataloader)}\")\n",
    "print(f\"Number of Testing batches: {len(testing_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89a6277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, input_size, activation=\"sin\", dropout_ratio=0.4):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        \n",
    "        # input size \n",
    "        self.input_size = input_size\n",
    "\n",
    "        \n",
    "        # non-periodic/linear vector \n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        # periodic/linear vector \n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "        # activation function \n",
    "        if activation == \"sin\": \n",
    "            self.activation = torch.sin\n",
    "        else:\n",
    "            self.activation = torch.cos\n",
    "            \n",
    "    def forward(self, x): \n",
    "        # periodic layer\n",
    "        out_periodic = self.fc1(x)\n",
    "        out_periodic = self.dropout(out_periodic)\n",
    "        \n",
    "        # non-periodic layer \n",
    "        out_nonperiodic = self.activation(self.fc2(x))\n",
    "        out_nonperiodic = self.dropout(out_nonperiodic)\n",
    "        \n",
    "        # output \n",
    "        out = torch.cat([out_periodic, out_nonperiodic], -1)\n",
    "        \n",
    "        return out \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fee29b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Time2Vector(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f5abe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters \n",
    "num_epoch = 8\n",
    "\n",
    "# model hyperparameters \n",
    "input_size = len(train_dataset_all[0][0][0])\n",
    "output_size = len(train_dataset_all[0][1])\n",
    "hidden_size = 256\n",
    "drop_out = 0.4\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "347415d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, input_size, activation=\"sin\", dropout_ratio=0.4):\n",
    "        super(Time2Vector, self).__init__()\n",
    "\n",
    "        # input size\n",
    "        self.input_size = input_size\n",
    "\n",
    "\n",
    "        # non-periodic/linear vector\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # periodic/linear vector\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        # activation function\n",
    "        if activation == \"sin\":\n",
    "            self.activation = torch.sin\n",
    "        else:\n",
    "            self.activation = torch.cos\n",
    "\n",
    "    def forward(self, x):\n",
    "        # periodic layer\n",
    "        out_periodic = self.fc1(x)\n",
    "        out_periodic = self.dropout(out_periodic)\n",
    "\n",
    "        # non-periodic layer\n",
    "        out_nonperiodic = self.activation(self.fc2(x))\n",
    "        out_nonperiodic = self.dropout(out_nonperiodic)\n",
    "\n",
    "        # output\n",
    "        out = torch.cat([out_periodic, out_nonperiodic], -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, output_size, kernel_size=1):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        # Time2Vector\n",
    "        self.t2v = Time2Vector(self.input_size)\n",
    "\n",
    "\n",
    "        # Conv1\n",
    "        self.conv1 = nn.Conv1d(self.input_size*3, (self.input_size*3)*4, kernel_size=kernel_size, bias=False)\n",
    "        self.conv2 = nn.Conv1d((self.input_size*3)*4, (self.input_size * 3) * 4, kernel_size=kernel_size, bias=False)\n",
    "\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM((self.input_size*3)*4, self.hidden_size, num_layers=self.num_layers, dropout=dropout_ratio,\n",
    "                            batch_first=True, bidirectional=True, bias=False)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, self.output_size)\n",
    "\n",
    "        # # BATCH_NORMALIZATION layer\n",
    "        self.batch1d1_conv1 = nn.BatchNorm1d((self.input_size*3)*4)\n",
    "        self.batch1d1_conv2 = nn.BatchNorm1d((self.input_size*3)*4)\n",
    "        self.batch1d2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.batch1d3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        # time_embeddings\n",
    "        out_embed = self.t2v(x)\n",
    "\n",
    "        # combine with input\n",
    "        x = torch.cat([x, out_embed], -1)\n",
    "\n",
    "        # get size of input to conv\n",
    "        batch_size, seq_len, features = x.size()\n",
    "\n",
    "        # reshape input to conv layer\n",
    "        x = x.view(batch_size * seq_len, features)\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # go throw conv layers\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d1_conv1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d1_conv2(x)\n",
    "\n",
    "        # reshape x for LSTM layer\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # lstm layer x[B, seq_len, input_size]\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # get the last output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d2(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d3(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, train_on_gpu):\n",
    "        '''\n",
    "        Initializes hidden state\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda(),\n",
    "                      weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0beceb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, input_size, activation=\"sin\", dropout_ratio=0.4):\n",
    "        super(Time2Vector, self).__init__()\n",
    "\n",
    "        # input size\n",
    "        self.input_size = input_size\n",
    "\n",
    "\n",
    "        # non-periodic/linear vector\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # periodic/linear vector\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        # activation function\n",
    "        if activation == \"sin\":\n",
    "            self.activation = torch.sin\n",
    "        else:\n",
    "            self.activation = torch.cos\n",
    "\n",
    "    def forward(self, x):\n",
    "        # periodic layer\n",
    "        out_periodic = self.fc1(x)\n",
    "        out_periodic = self.dropout(out_periodic)\n",
    "\n",
    "        # non-periodic layer\n",
    "        out_nonperiodic = self.activation(self.fc2(x))\n",
    "        out_nonperiodic = self.dropout(out_nonperiodic)\n",
    "\n",
    "        # output\n",
    "        out = torch.cat([out_periodic, out_nonperiodic], -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, output_size, kernel_size=1):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        # Time2Vector\n",
    "        self.t2v = Time2Vector(self.input_size)\n",
    "\n",
    "\n",
    "        # Conv1\n",
    "        self.conv1 = nn.Conv1d(self.input_size*3, (self.input_size*3)*4, kernel_size=kernel_size, bias=False)\n",
    "        self.conv2 = nn.Conv1d((self.input_size*3)*4, (self.input_size * 3) * 4, kernel_size=kernel_size, bias=False)\n",
    "\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM((self.input_size*3)*4, self.hidden_size, num_layers=self.num_layers, dropout=dropout_ratio,\n",
    "                            batch_first=True, bidirectional=True, bias=False)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, self.output_size)\n",
    "\n",
    "        # # BATCH_NORMALIZATION layer\n",
    "        self.batch1d1_conv1 = nn.BatchNorm1d((self.input_size*3)*4)\n",
    "        self.batch1d1_conv2 = nn.BatchNorm1d((self.input_size*3)*4)\n",
    "        self.batch1d2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.batch1d3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        # time_embeddings\n",
    "        out_embed = self.t2v(x)\n",
    "\n",
    "        # combine with input\n",
    "        x = torch.cat([x, out_embed], -1)\n",
    "\n",
    "        # get size of input to conv\n",
    "        batch_size, seq_len, features = x.size()\n",
    "\n",
    "        # reshape input to conv layer\n",
    "        x = x.view(batch_size * seq_len, features)\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # go throw conv layers\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d1_conv1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d1_conv2(x)\n",
    "\n",
    "        # reshape x for LSTM layer\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # lstm layer x[B, seq_len, input_size]\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # get the last output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d2(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d3(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, train_on_gpu):\n",
    "        '''\n",
    "        Initializes hidden state\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda(),\n",
    "                      weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19d8d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_BiLSTM(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (t2v): Time2Vector(\n",
       "    (fc1): Linear(in_features=13, out_features=13, bias=True)\n",
       "    (fc2): Linear(in_features=13, out_features=13, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (conv1): Conv1d(39, 156, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (conv2): Conv1d(156, 156, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (lstm): LSTM(156, 256, num_layers=2, bias=False, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=False)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (batch1d1_conv1): BatchNorm1d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d1_conv2): BatchNorm1d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the network\n",
    "model = CNN_BiLSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae54409c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3904/3354042004.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3904/2685738664.py\u001b[0m in \u001b[0;36minit_hidden\u001b[1;34m(self, batch_size, train_on_gpu)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_on_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             hidden = (weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda(),\n\u001b[0m\u001b[0;32m    140\u001b[0m                       weight.new(self.num_layers*2, batch_size, self.hidden_size).zero_().cuda())\n\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Crypto_Bot_Project\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model.init_hidden(64, True)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "700fcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_dataloader, validation_dataloader, epochs=10, clip=2, print_every=200, learning_rate_decay=1800):\n",
    "    step = 0 \n",
    "    model.train()\n",
    "    if(train_on_gpu):\n",
    "        model.cuda()\n",
    "    counter = 0 \n",
    "    # start at inf for val_loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    # lr set at the beginning to be high at 0.1\n",
    "    lr = 0.01\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    print(f\"learning_rate: {lr}\")\n",
    "    \n",
    "    # start training loop\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size, train_on_gpu)\n",
    "        train_losses = []\n",
    "        for inputs, targets in training_dataloader:\n",
    "            counter += 1\n",
    "            \n",
    "            # decrease the learning rate every 1500 step \n",
    "            if counter % learning_rate_decay == 0: \n",
    "                # decrease the lr by 0.25 each 1500 step to help model converge \n",
    "                lr= lr * 0.775\n",
    "                opt = optim.SGD(model.parameters(), lr=lr)\n",
    "                print(f\"learning_rate decreased to: {lr}\")\n",
    "                \n",
    "            \n",
    "            # move to GPU if available   \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get network output\n",
    "            output, h = model(inputs, h)\n",
    "            # find loss and back propogate \n",
    "            loss = criterion(output, targets.float())\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, targets in validation_dataloader: \n",
    "                    \n",
    "                    # move to GPU\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    # create new hidden varaibles \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    # forward \n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    \n",
    "                    # calculate val_batch accuracy \n",
    "                    \n",
    "                    # loss\n",
    "                    val_loss = criterion(output, targets.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                # average val loss over all batches \n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "                train_losses = []\n",
    "                \n",
    "                # save model if validation loss has decreased\n",
    "                if avg_val_loss <= valid_loss_min:\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    valid_loss_min,\n",
    "                    avg_val_loss))\n",
    "                    torch.save(model.state_dict(), \"General_Model_price\")\n",
    "                    valid_loss_min = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a480b018",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15232/516180991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e79a13ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"CNN_BiLSTM_Model_price.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab17fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "val_losses = []\n",
    "model.eval()\n",
    "output_list, targets_list = [], []\n",
    "for inputs, targets in validation_dataloader: \n",
    "\n",
    "    # move to GPU\n",
    "    if(train_on_gpu):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    # create new hidden varaibles \n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "    # forward \n",
    "    output, val_h = model(inputs, val_h)\n",
    "\n",
    "    # calculate val_batch accuracy \n",
    "    output_list.append(output)\n",
    "    targets_list.append(targets)\n",
    "    # loss\n",
    "    val_loss = criterion(output, targets.float())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "model.train()\n",
    "\n",
    "# average val loss over all batches \n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b19a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
