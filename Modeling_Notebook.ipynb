{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131050bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required lib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from utils import pct_change, preprocess_and_split_df, convert_to_dataset, data_preparation, plot_data, create_bollinger_limits\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import random\n",
    "import torch\n",
    "import statistics\n",
    "from LSTM_model_future_change import LSTM_FutureChangeGeneral, LSTM_FutureChange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09538dc",
   "metadata": {},
   "source": [
    "## Upload the dataframes ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe8a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data dict again \n",
    "CRYPTO_DICT = { \n",
    "            \"ADA\": {}, \n",
    "            \"BCH\": {},\n",
    "            \"BNB\": {}, \n",
    "            \"BTC\": {},\n",
    "            \"ETH\": {},\n",
    "            \"LTC\": {}, \n",
    "            \"NEO\": {},\n",
    "            \"TRX\": {},\n",
    "            \"XRP\": {},\n",
    "}\n",
    "\n",
    "# upload the data \n",
    "\n",
    "for Crypto_code in CRYPTO_DICT:\n",
    "    CRYPTO_DICT[Crypto_code][\"df\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-main_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_train\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-train_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_val\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-val_df.pkl\")\n",
    "    CRYPTO_DICT[Crypto_code][\"df_test\"]  = pd.read_pickle(f\"Training_Data/Data_preprocessed/{Crypto_code}/{Crypto_code}-test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b43a859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unix', 'date', 'symbol', 'close', 'mid_price_1', 'mid_price_2',\n",
       "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
       "       'upper_check', 'lower_check', 'future_price', 'future_change'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRYPTO_DICT[\"BTC\"][\"df_train\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435797f",
   "metadata": {},
   "source": [
    "## Create Datasets for all coins ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3da73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for future price prediction \n",
    "future_price_columns =  ['close', 'mid_price_1', 'mid_price_2',\n",
    "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
    "       'upper_check', 'lower_check', 'future_price']\n",
    "\n",
    "future_price_length = (len(future_price_columns) - 1)\n",
    "\n",
    "# Dataset for future change prediction \n",
    "future_change_columns = ['close', 'mid_price_1', 'mid_price_2',\n",
    "       'Volume USDT', 'SMA', 'EMA0', 'EMA1', 'EMA2', 'RSI', 'upper', 'lower',\n",
    "       'upper_check', 'lower_check','future_change']\n",
    "future_change_length = (len(future_change_columns) -  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d07660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_price_dataset start converting for all coins\n",
      "future_price_dataset finished converting for all coins\n",
      "future_price_dataset start converting for all coins\n",
      "future_price_dataset finished converting for all coins\n"
     ]
    }
   ],
   "source": [
    "datasets = [[\"future_price_dataset\", future_price_columns, future_price_length], [\"future_change_dataset\", future_change_columns, future_change_length]]\n",
    "for dataset in datasets: \n",
    "    print(f\"{datasets[0][0]} start converting for all coins\")\n",
    "    for Crypto_code in CRYPTO_DICT:\n",
    "        if \"datasets\" not in list(CRYPTO_DICT[Crypto_code].keys()):\n",
    "            CRYPTO_DICT[Crypto_code][\"datasets\"] = {}\n",
    "        for df in CRYPTO_DICT[Crypto_code]: \n",
    "            if df == \"df\" or df == \"datasets\": \n",
    "                pass\n",
    "            else:\n",
    "                if dataset[0] not in list(CRYPTO_DICT[Crypto_code][\"datasets\"].keys()):\n",
    "                    CRYPTO_DICT[Crypto_code][\"datasets\"][dataset[0]] = {}\n",
    "                CRYPTO_DICT[Crypto_code][\"datasets\"][dataset[0]][f\"{df}_dataset\"] = convert_to_dataset(CRYPTO_DICT[Crypto_code][df], dataset[1], dataset[2])\n",
    "    print(f\"{datasets[0][0]} finished converting for all coins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2a3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26733"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_change_dataset']['df_test_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c75079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24823"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CRYPTO_DICT[\"BTC\"][\"datasets\"]['future_change_dataset']['df_test_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4cbfef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA\n",
      "BCH\n",
      "BNB\n",
      "BTC\n",
      "ETH\n",
      "LTC\n",
      "NEO\n",
      "TRX\n",
      "XRP\n"
     ]
    }
   ],
   "source": [
    "for key in CRYPTO_DICT: \n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbe354",
   "metadata": {},
   "source": [
    "# Modeling section #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e0851",
   "metadata": {},
   "source": [
    "## 1. LSTM_MODEL ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a08f48",
   "metadata": {},
   "source": [
    "### 1.1 Future_change_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969299bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model creation lib\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a8faa",
   "metadata": {},
   "source": [
    "#### Dataloaders and Hyperparameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "953634b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating datasets for one coin \n",
    "# ADA \n",
    "# Process will be same for other coins \n",
    "train_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_change_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_change_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_change_dataset']['df_test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d45f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters \n",
    "num_epoch = 6\n",
    "batch_size = 64\n",
    "\n",
    "# model hyperparameters \n",
    "input_size = len(train_dataset[0][0][0])\n",
    "output_size = len(train_dataset[0][1])\n",
    "hidden_size = 128\n",
    "drop_out = 0.4\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c2ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3348"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloader for all data types\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "len(training_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd7554",
   "metadata": {},
   "source": [
    "#### Creating the model and checking for GPU ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ea32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e11e9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard to get nice loss plot\n",
    "writer_train = SummaryWriter(f\"runs/loss_plot\")\n",
    "writer_val = SummaryWriter(f\"runs/val_plot\")\n",
    "writer_acc = SummaryWriter(f\"runs/acc_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8d106a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch1d2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5bdd4",
   "metadata": {},
   "source": [
    "#### Training loop and accuracy calculation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64170475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_get(targets, output):\n",
    "    output_round = torch.round(output)\n",
    "    acc = 0\n",
    "    for i in range(len(targets)): \n",
    "        if targets[i] == output_round[i]: \n",
    "            acc += 1 \n",
    "    acc_pct = (acc) / len(targets)\n",
    "    return acc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a03d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, Crypto_code, training_dataloader, validation_dataloader, epochs=10, clip=2, print_every=200, learning_rate_decay=1400):\n",
    "    step = 0 \n",
    "    model.train()\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    counter = 0 \n",
    "    # start at inf for val_loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    # lr set at the beginning to be high at 0.1\n",
    "    lr = 0.1\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    print(f\"learning_rate: {lr}\")\n",
    "    \n",
    "    # start training loop\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size, train_on_gpu)\n",
    "        \n",
    "        for inputs, targets in training_dataloader:\n",
    "            counter += 1\n",
    "            \n",
    "            # decrease the learning rate every 1500 step \n",
    "            if counter % learning_rate_decay == 0: \n",
    "                # decrease the lr by 0.25 each 1500 step to help model converge \n",
    "                lr= lr * 0.725\n",
    "                opt = optim.SGD(model.parameters(), lr=lr)\n",
    "                print(f\"learning_rate decreased to: {lr}\")\n",
    "                \n",
    "            \n",
    "            # move to GPU if available   \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get network output\n",
    "            output, hidden = model(inputs, h)\n",
    "            # find loss and back propogate \n",
    "            loss = criterion(output, targets.float())\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_acc = []\n",
    "                val_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, targets in validation_dataloader: \n",
    "                    \n",
    "                    # move to GPU\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    # create new hidden varaibles \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    # forward \n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    \n",
    "                    # calculate val_batch accuracy \n",
    "                    val_acc.append(acc_get(targets, output))\n",
    "                    \n",
    "                    # loss\n",
    "                    val_loss = criterion(output, targets.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                # average val loss over all batches \n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                # avg val accuracy over all batches \n",
    "                avg_val_acc = statistics.mean(val_acc)\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                  \"val_acc: {:.6f}\".format(avg_val_acc))\n",
    "                    \n",
    "                    \n",
    "                writer_train.add_scalar('training loss', loss.item(), global_step=step)\n",
    "                writer_val.add_scalar('val_loss', avg_val_loss, global_step=step)\n",
    "                writer_acc.add_scalar('val_accuracy', avg_val_acc, global_step=step)\n",
    "                step += 1\n",
    "                \n",
    "                # save model if validation loss has decreased\n",
    "                if avg_val_loss <= valid_loss_min:\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    valid_loss_min,\n",
    "                    avg_val_loss))\n",
    "                    torch.save(model.state_dict(), f\"Trained-models/LSTM-model/{Crypto_code}-LSTM_model.pt\")\n",
    "                    valid_loss_min = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462686d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2c9fc77",
   "metadata": {},
   "source": [
    "### Model Testing on Testing Datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256a08b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the general model to be compared by each coin-specific model \n",
    "MAIN_MODEL = LSTM_FutureChangeGeneral(input_size, 160, num_layers, 0.45, output_size).float()\n",
    "MAIN_MODEL.load_state_dict(torch.load(\"Trained-models/LSTM-model/General-LSTM_model.pt\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3a166fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Start testing for ADA ########\n",
      "Number of batches for ADA-Coin: 3348\n",
      "test losses for ADA using General_Model: 0.2993612183099935\n",
      "test accuracy for ADA using General_Model: 89.96602449223417%\n",
      "test losses for ADA using coin_specific_model: 0.32181599406412\n",
      "test accuracy for ADA using coin_specific_model: 88.49919728195937%\n",
      "###### Start testing for BCH ########\n",
      "Number of batches for BCH-Coin: 2598\n",
      "test losses for BCH using General_Model: 0.4905321517180626\n",
      "test accuracy for BCH using General_Model: 81.20729888375674%\n",
      "test losses for BCH using coin_specific_model: 0.45796974766153475\n",
      "test accuracy for BCH using coin_specific_model: 78.99345650500385%\n",
      "###### Start testing for BNB ########\n",
      "Number of batches for BNB-Coin: 2572\n",
      "test losses for BNB using General_Model: 0.2788405732293707\n",
      "test accuracy for BNB using General_Model: 90.76533339813375%\n",
      "test losses for BNB using coin_specific_model: 0.28445740154774896\n",
      "test accuracy for BNB using coin_specific_model: 89.5922433903577%\n",
      "###### Start testing for BTC ########\n",
      "Number of batches for BTC-Coin: 3109\n",
      "test losses for BTC using General_Model: 0.23690656832835422\n",
      "test accuracy for BTC using General_Model: 92.68906802830492%\n",
      "test losses for BTC using coin_specific_model: 0.2249031881977025\n",
      "test accuracy for BTC using coin_specific_model: 92.57448134448376%\n",
      "###### Start testing for ETH ########\n",
      "Number of batches for ETH-Coin: 2396\n",
      "test losses for ETH using General_Model: 0.2319447846899745\n",
      "test accuracy for ETH using General_Model: 92.87875626043406%\n",
      "test losses for ETH using coin_specific_model: 0.2278030480325023\n",
      "test accuracy for ETH using coin_specific_model: 92.38248643572621%\n",
      "###### Start testing for LTC ########\n",
      "Number of batches for LTC-Coin: 2641\n",
      "test losses for LTC using General_Model: 0.25329287195862443\n",
      "test accuracy for LTC using General_Model: 92.01533510034078%\n",
      "test losses for LTC using coin_specific_model: 0.24383517330465151\n",
      "test accuracy for LTC using coin_specific_model: 91.70531995456267%\n",
      "###### Start testing for NEO ########\n",
      "Number of batches for NEO-Coin: 1650\n",
      "test losses for NEO using General_Model: 0.510839586890105\n",
      "test accuracy for NEO using General_Model: 80.38731060606061%\n",
      "test losses for NEO using coin_specific_model: 0.47449173885764495\n",
      "test accuracy for NEO using coin_specific_model: 77.70359848484848%\n",
      "###### Start testing for TRX ########\n",
      "Number of batches for TRX-Coin: 1682\n",
      "test losses for TRX using General_Model: 0.23741814969428654\n",
      "test accuracy for TRX using General_Model: 92.7374405469679%\n",
      "test losses for TRX using coin_specific_model: 0.23536290458705847\n",
      "test accuracy for TRX using coin_specific_model: 91.72488109393579%\n",
      "###### Start testing for XRP ########\n",
      "Number of batches for XRP-Coin: 1771\n",
      "test losses for XRP using General_Model: 0.228585953457688\n",
      "test accuracy for XRP using General_Model: 93.21534443817052%\n",
      "test losses for XRP using coin_specific_model: 0.2299118230675722\n",
      "test accuracy for XRP using coin_specific_model: 91.72519057029926%\n"
     ]
    }
   ],
   "source": [
    "# Loop for test data evaulation for each coin \n",
    "models = [MAIN_MODEL, None]\n",
    "criterion = nn.BCELoss()\n",
    "for Crypto_code in CRYPTO_DICT:\n",
    "    print(f\"###### Start testing for {Crypto_code} ########\")\n",
    "    \n",
    "    # Create the Dataloaders \n",
    "    test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_change_dataset']['df_train_dataset']\n",
    "    testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    print(f\"Number of batches for {Crypto_code}-Coin: {len(testing_dataloader)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # upload the coin specific model \n",
    "    coin_model = LSTM_FutureChange(input_size, hidden_size, num_layers, drop_out, output_size).float()\n",
    "    coin_model.load_state_dict(torch.load(f\"Trained-models/LSTM-model/{Crypto_code}-LSTM_model.pt\"))\n",
    "    models[1] = coin_model\n",
    "    \n",
    "    # start testing for each model\n",
    "    for i_model, (model) in enumerate(models): \n",
    "        test_acc = []\n",
    "        test_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "        test_losses = []\n",
    "        model.eval()\n",
    "        for i, (inputs, targets) in enumerate(testing_dataloader): \n",
    "        #     print(i)\n",
    "            # move to GPU\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # create new hidden varaibles \n",
    "            test_h = tuple([each.data for each in test_h])\n",
    "\n",
    "            # forward \n",
    "            output, test_h = model(inputs, test_h)\n",
    "\n",
    "            # calculate val_batch accuracy \n",
    "            test_acc.append(acc_get(targets, output))\n",
    "\n",
    "            # loss\n",
    "            test_loss = criterion(output, targets.float())\n",
    "            test_losses.append(test_loss.item())\n",
    "        \n",
    "        if i_model == 0: \n",
    "        \n",
    "            # average val loss over all batches \n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "            # avg val accuracy over all batches \n",
    "            avg_test_acc = statistics.mean(test_acc)\n",
    "            print(f\"test losses for {Crypto_code} using General_Model: {avg_test_loss}\")\n",
    "            print(f\"test accuracy for {Crypto_code} using General_Model: {avg_test_acc * 100}%\")\n",
    "        else: \n",
    "            # average val loss over all batches \n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "            # avg val accuracy over all batches \n",
    "            avg_test_acc = statistics.mean(test_acc)\n",
    "            print(f\"test losses for {Crypto_code} using coin_specific_model: {avg_test_loss}\")\n",
    "            print(f\"test accuracy for {Crypto_code} using coin_specific_model: {avg_test_acc * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "492b114d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba55764",
   "metadata": {},
   "source": [
    "## 2. Model using future price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a198bef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CRYPTO_DICT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13176/2897245942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# ADA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Process will be same for other coins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRYPTO_DICT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ADA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datasets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'future_price_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'df_train_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRYPTO_DICT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ADA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datasets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'future_price_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'df_val_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRYPTO_DICT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ADA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datasets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'future_price_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'df_test_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CRYPTO_DICT' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of creating datasets for one coin \n",
    "# ADA \n",
    "# Process will be same for other coins \n",
    "train_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[\"ADA\"][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96eea2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13176/110843054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# model hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# training hyperparameters \n",
    "num_epoch = 8\n",
    "batch_size = 64\n",
    "\n",
    "# model hyperparameters \n",
    "input_size = len(train_dataset[0][0][0])\n",
    "output_size = len(train_dataset[0][1])\n",
    "hidden_size = 128\n",
    "drop_out = 0.4\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66ffd279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3348"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloader for all data types\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "len(training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44ff7a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f76631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=dropout_ratio,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, self.output_size)\n",
    "\n",
    "        # # BATCH_NORMALIZATION layer\n",
    " \n",
    "        self.batch1d2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batch1d3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # lstm layer x[B, seq_len, input_size]\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # get the last output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d2(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "\n",
    "        # drop out and batch_norm\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch1d3(x)\n",
    "\n",
    "        # first fc layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, train_on_gpu):\n",
    "        '''\n",
    "        Initializes hidden state\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda(),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f2a7b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch1d2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75152acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, Crypto_code, training_dataloader, validation_dataloader, epochs=10, clip=2, print_every=200, learning_rate_decay=1800):\n",
    "    step = 0 \n",
    "    model.train()\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    counter = 0 \n",
    "    # start at inf for val_loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    # lr set at the beginning to be high at 0.1\n",
    "    lr = 0.01\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    print(f\"learning_rate: {lr}\")\n",
    "    \n",
    "    # start training loop\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size, train_on_gpu)\n",
    "        \n",
    "        for inputs, targets in training_dataloader:\n",
    "            counter += 1\n",
    "            \n",
    "            # decrease the learning rate every 1500 step \n",
    "            if counter % learning_rate_decay == 0: \n",
    "                # decrease the lr by 0.25 each 1500 step to help model converge \n",
    "                lr= lr * 0.775\n",
    "                opt = optim.SGD(model.parameters(), lr=lr)\n",
    "                print(f\"learning_rate decreased to: {lr}\")\n",
    "                \n",
    "            \n",
    "            # move to GPU if available   \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get network output\n",
    "            output, hidden = model(inputs, h)\n",
    "            # find loss and back propogate \n",
    "            loss = criterion(output, targets.float())\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, targets in validation_dataloader: \n",
    "                    \n",
    "                    # move to GPU\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    # create new hidden varaibles \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    # forward \n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    \n",
    "                    # calculate val_batch accuracy \n",
    "                    \n",
    "                    # loss\n",
    "                    val_loss = criterion(output, targets.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                # average val loss over all batches \n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                    \n",
    "                \n",
    "                # save model if validation loss has decreased\n",
    "                if avg_val_loss <= valid_loss_min:\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    valid_loss_min,\n",
    "                    avg_val_loss))\n",
    "                    torch.save(model.state_dict(), f\"Trained-models/LSTM-model/Future_price/{Crypto_code}_price_LSTM_model.pt\")\n",
    "                    valid_loss_min = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dea9c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.01\n",
      "Epoch: 1/8... Step: 200... Loss: 1.352781... Val Loss: 0.649328\n",
      "Validation loss decreased (inf --> 0.649328).  Saving model ...\n",
      "Epoch: 1/8... Step: 400... Loss: 0.932932... Val Loss: 0.649976\n",
      "Epoch: 1/8... Step: 600... Loss: 0.635322... Val Loss: 0.650135\n",
      "Epoch: 1/8... Step: 800... Loss: 0.763163... Val Loss: 0.649900\n",
      "Epoch: 1/8... Step: 1000... Loss: 1.297694... Val Loss: 0.648800\n",
      "Validation loss decreased (0.649328 --> 0.648800).  Saving model ...\n",
      "Epoch: 1/8... Step: 1200... Loss: 0.958415... Val Loss: 0.654124\n",
      "Epoch: 1/8... Step: 1400... Loss: 0.871864... Val Loss: 0.649502\n",
      "Epoch: 1/8... Step: 1600... Loss: 1.301817... Val Loss: 0.649969\n",
      "learning_rate decreased to: 0.007750000000000001\n",
      "Epoch: 1/8... Step: 1800... Loss: 0.911590... Val Loss: 0.650423\n",
      "Epoch: 1/8... Step: 2000... Loss: 0.728262... Val Loss: 0.649902\n",
      "Epoch: 1/8... Step: 2200... Loss: 0.876958... Val Loss: 0.649242\n",
      "Epoch: 1/8... Step: 2400... Loss: 0.920017... Val Loss: 0.649658\n",
      "Epoch: 1/8... Step: 2600... Loss: 1.643940... Val Loss: 0.649417\n",
      "Epoch: 1/8... Step: 2800... Loss: 0.898282... Val Loss: 0.649602\n",
      "Epoch: 1/8... Step: 3000... Loss: 0.853600... Val Loss: 0.649396\n",
      "Epoch: 1/8... Step: 3200... Loss: 0.893284... Val Loss: 0.650179\n",
      "Epoch: 2/8... Step: 3400... Loss: 1.265937... Val Loss: 0.649804\n",
      "learning_rate decreased to: 0.006006250000000001\n",
      "Epoch: 2/8... Step: 3600... Loss: 1.040365... Val Loss: 0.649950\n",
      "Epoch: 2/8... Step: 3800... Loss: 1.013879... Val Loss: 0.650074\n",
      "Epoch: 2/8... Step: 4000... Loss: 1.323625... Val Loss: 0.651076\n",
      "Epoch: 2/8... Step: 4200... Loss: 0.998044... Val Loss: 0.650650\n",
      "Epoch: 2/8... Step: 4400... Loss: 0.850159... Val Loss: 0.650662\n",
      "Epoch: 2/8... Step: 4600... Loss: 1.205065... Val Loss: 0.651289\n",
      "Epoch: 2/8... Step: 4800... Loss: 0.946797... Val Loss: 0.650646\n",
      "Epoch: 2/8... Step: 5000... Loss: 1.138716... Val Loss: 0.649126\n",
      "Epoch: 2/8... Step: 5200... Loss: 1.374648... Val Loss: 0.653909\n",
      "learning_rate decreased to: 0.004654843750000001\n",
      "Epoch: 2/8... Step: 5400... Loss: 1.395306... Val Loss: 0.651157\n",
      "Epoch: 2/8... Step: 5600... Loss: 1.037870... Val Loss: 0.650296\n",
      "Epoch: 2/8... Step: 5800... Loss: 1.148076... Val Loss: 0.651073\n",
      "Epoch: 2/8... Step: 6000... Loss: 1.037897... Val Loss: 0.649437\n",
      "Epoch: 2/8... Step: 6200... Loss: 0.966678... Val Loss: 0.648873\n",
      "Epoch: 2/8... Step: 6400... Loss: 1.188852... Val Loss: 0.652629\n",
      "Epoch: 2/8... Step: 6600... Loss: 0.926749... Val Loss: 0.650115\n",
      "Epoch: 3/8... Step: 6800... Loss: 1.048245... Val Loss: 0.651006\n",
      "Epoch: 3/8... Step: 7000... Loss: 0.946926... Val Loss: 0.650923\n",
      "learning_rate decreased to: 0.003607503906250001\n",
      "Epoch: 3/8... Step: 7200... Loss: 1.032459... Val Loss: 0.648883\n",
      "Epoch: 3/8... Step: 7400... Loss: 1.506734... Val Loss: 0.648623\n",
      "Validation loss decreased (0.648800 --> 0.648623).  Saving model ...\n",
      "Epoch: 3/8... Step: 7600... Loss: 0.816157... Val Loss: 0.649755\n",
      "Epoch: 3/8... Step: 7800... Loss: 1.402613... Val Loss: 0.652275\n",
      "Epoch: 3/8... Step: 8000... Loss: 0.668805... Val Loss: 0.648780\n",
      "Epoch: 3/8... Step: 8200... Loss: 0.872723... Val Loss: 0.648525\n",
      "Validation loss decreased (0.648623 --> 0.648525).  Saving model ...\n",
      "Epoch: 3/8... Step: 8400... Loss: 1.331236... Val Loss: 0.648609\n",
      "Epoch: 3/8... Step: 8600... Loss: 1.035485... Val Loss: 0.648832\n",
      "Epoch: 3/8... Step: 8800... Loss: 0.985686... Val Loss: 0.648583\n",
      "learning_rate decreased to: 0.002795815527343751\n",
      "Epoch: 3/8... Step: 9000... Loss: 1.211544... Val Loss: 0.645940\n",
      "Validation loss decreased (0.648525 --> 0.645940).  Saving model ...\n",
      "Epoch: 3/8... Step: 9200... Loss: 0.885546... Val Loss: 0.647902\n",
      "Epoch: 3/8... Step: 9400... Loss: 1.015196... Val Loss: 0.648395\n",
      "Epoch: 3/8... Step: 9600... Loss: 1.175665... Val Loss: 0.641514\n",
      "Validation loss decreased (0.645940 --> 0.641514).  Saving model ...\n",
      "Epoch: 3/8... Step: 9800... Loss: 0.918276... Val Loss: 0.640626\n",
      "Validation loss decreased (0.641514 --> 0.640626).  Saving model ...\n",
      "Epoch: 3/8... Step: 10000... Loss: 0.884344... Val Loss: 0.642853\n",
      "Epoch: 4/8... Step: 10200... Loss: 1.012467... Val Loss: 0.622542\n",
      "Validation loss decreased (0.640626 --> 0.622542).  Saving model ...\n",
      "Epoch: 4/8... Step: 10400... Loss: 1.325398... Val Loss: 0.618162\n",
      "Validation loss decreased (0.622542 --> 0.618162).  Saving model ...\n",
      "Epoch: 4/8... Step: 10600... Loss: 1.065488... Val Loss: 0.616722\n",
      "Validation loss decreased (0.618162 --> 0.616722).  Saving model ...\n",
      "learning_rate decreased to: 0.002166757033691407\n",
      "Epoch: 4/8... Step: 10800... Loss: 0.798575... Val Loss: 0.609058\n",
      "Validation loss decreased (0.616722 --> 0.609058).  Saving model ...\n",
      "Epoch: 4/8... Step: 11000... Loss: 0.985762... Val Loss: 0.601368\n",
      "Validation loss decreased (0.609058 --> 0.601368).  Saving model ...\n",
      "Epoch: 4/8... Step: 11200... Loss: 0.951692... Val Loss: 0.614608\n",
      "Epoch: 4/8... Step: 11400... Loss: 1.140619... Val Loss: 0.609805\n",
      "Epoch: 4/8... Step: 11600... Loss: 0.652929... Val Loss: 0.581920\n",
      "Validation loss decreased (0.601368 --> 0.581920).  Saving model ...\n",
      "Epoch: 4/8... Step: 11800... Loss: 1.129829... Val Loss: 0.579963\n",
      "Validation loss decreased (0.581920 --> 0.579963).  Saving model ...\n",
      "Epoch: 4/8... Step: 12000... Loss: 0.976874... Val Loss: 0.571035\n",
      "Validation loss decreased (0.579963 --> 0.571035).  Saving model ...\n",
      "Epoch: 4/8... Step: 12200... Loss: 0.930937... Val Loss: 0.578982\n",
      "Epoch: 4/8... Step: 12400... Loss: 0.975724... Val Loss: 0.556403\n",
      "Validation loss decreased (0.571035 --> 0.556403).  Saving model ...\n",
      "learning_rate decreased to: 0.0016792367011108404\n",
      "Epoch: 4/8... Step: 12600... Loss: 1.139962... Val Loss: 0.565924\n",
      "Epoch: 4/8... Step: 12800... Loss: 0.626606... Val Loss: 0.580559\n",
      "Epoch: 4/8... Step: 13000... Loss: 0.882461... Val Loss: 0.539150\n",
      "Validation loss decreased (0.556403 --> 0.539150).  Saving model ...\n",
      "Epoch: 4/8... Step: 13200... Loss: 0.850711... Val Loss: 0.540943\n",
      "Epoch: 5/8... Step: 13400... Loss: 0.951307... Val Loss: 0.531712\n",
      "Validation loss decreased (0.539150 --> 0.531712).  Saving model ...\n",
      "Epoch: 5/8... Step: 13600... Loss: 0.828420... Val Loss: 0.528414\n",
      "Validation loss decreased (0.531712 --> 0.528414).  Saving model ...\n",
      "Epoch: 5/8... Step: 13800... Loss: 1.107480... Val Loss: 0.526894\n",
      "Validation loss decreased (0.528414 --> 0.526894).  Saving model ...\n",
      "Epoch: 5/8... Step: 14000... Loss: 1.228700... Val Loss: 0.524139\n",
      "Validation loss decreased (0.526894 --> 0.524139).  Saving model ...\n",
      "Epoch: 5/8... Step: 14200... Loss: 0.849801... Val Loss: 0.519856\n",
      "Validation loss decreased (0.524139 --> 0.519856).  Saving model ...\n",
      "learning_rate decreased to: 0.0013014084433609014\n",
      "Epoch: 5/8... Step: 14400... Loss: 0.776138... Val Loss: 0.523170\n",
      "Epoch: 5/8... Step: 14600... Loss: 1.229350... Val Loss: 0.526328\n",
      "Epoch: 5/8... Step: 14800... Loss: 1.060634... Val Loss: 0.520397\n",
      "Epoch: 5/8... Step: 15000... Loss: 0.848552... Val Loss: 0.531421\n",
      "Epoch: 5/8... Step: 15200... Loss: 1.346020... Val Loss: 0.512425\n",
      "Validation loss decreased (0.519856 --> 0.512425).  Saving model ...\n",
      "Epoch: 5/8... Step: 15400... Loss: 1.221987... Val Loss: 0.515795\n",
      "Epoch: 5/8... Step: 15600... Loss: 1.255870... Val Loss: 0.527263\n",
      "Epoch: 5/8... Step: 15800... Loss: 0.840851... Val Loss: 0.511888\n",
      "Validation loss decreased (0.512425 --> 0.511888).  Saving model ...\n",
      "Epoch: 5/8... Step: 16000... Loss: 1.417431... Val Loss: 0.516493\n",
      "learning_rate decreased to: 0.0010085915436046987\n",
      "Epoch: 5/8... Step: 16200... Loss: 1.075138... Val Loss: 0.503406\n",
      "Validation loss decreased (0.511888 --> 0.503406).  Saving model ...\n",
      "Epoch: 5/8... Step: 16400... Loss: 0.871116... Val Loss: 0.520662\n",
      "Epoch: 5/8... Step: 16600... Loss: 0.900238... Val Loss: 0.503375\n",
      "Validation loss decreased (0.503406 --> 0.503375).  Saving model ...\n",
      "Epoch: 6/8... Step: 16800... Loss: 0.814057... Val Loss: 0.504854\n",
      "Epoch: 6/8... Step: 17000... Loss: 0.767734... Val Loss: 0.496611\n",
      "Validation loss decreased (0.503375 --> 0.496611).  Saving model ...\n",
      "Epoch: 6/8... Step: 17200... Loss: 0.981201... Val Loss: 0.496539\n",
      "Validation loss decreased (0.496611 --> 0.496539).  Saving model ...\n",
      "Epoch: 6/8... Step: 17400... Loss: 1.220688... Val Loss: 0.518964\n",
      "Epoch: 6/8... Step: 17600... Loss: 1.345691... Val Loss: 0.492686\n",
      "Validation loss decreased (0.496539 --> 0.492686).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/8... Step: 17800... Loss: 0.792146... Val Loss: 0.489002\n",
      "Validation loss decreased (0.492686 --> 0.489002).  Saving model ...\n",
      "learning_rate decreased to: 0.0007816584462936415\n",
      "Epoch: 6/8... Step: 18000... Loss: 0.638651... Val Loss: 0.489036\n",
      "Epoch: 6/8... Step: 18200... Loss: 0.882995... Val Loss: 0.484895\n",
      "Validation loss decreased (0.489002 --> 0.484895).  Saving model ...\n",
      "Epoch: 6/8... Step: 18400... Loss: 0.984945... Val Loss: 0.484580\n",
      "Validation loss decreased (0.484895 --> 0.484580).  Saving model ...\n",
      "Epoch: 6/8... Step: 18600... Loss: 0.724240... Val Loss: 0.485858\n",
      "Epoch: 6/8... Step: 18800... Loss: 0.717596... Val Loss: 0.484665\n",
      "Epoch: 6/8... Step: 19000... Loss: 0.971033... Val Loss: 0.482775\n",
      "Validation loss decreased (0.484580 --> 0.482775).  Saving model ...\n",
      "Epoch: 6/8... Step: 19200... Loss: 0.740823... Val Loss: 0.485715\n",
      "Epoch: 6/8... Step: 19400... Loss: 0.912307... Val Loss: 0.488895\n",
      "Epoch: 6/8... Step: 19600... Loss: 0.718699... Val Loss: 0.484372\n",
      "learning_rate decreased to: 0.0006057852958775722\n",
      "Epoch: 6/8... Step: 19800... Loss: 0.585749... Val Loss: 0.483746\n",
      "Epoch: 6/8... Step: 20000... Loss: 0.842280... Val Loss: 0.477874\n",
      "Validation loss decreased (0.482775 --> 0.477874).  Saving model ...\n",
      "Epoch: 7/8... Step: 20200... Loss: 0.764687... Val Loss: 0.476025\n",
      "Validation loss decreased (0.477874 --> 0.476025).  Saving model ...\n",
      "Epoch: 7/8... Step: 20400... Loss: 1.065826... Val Loss: 0.484399\n",
      "Epoch: 7/8... Step: 20600... Loss: 0.663142... Val Loss: 0.477327\n",
      "Epoch: 7/8... Step: 20800... Loss: 0.796899... Val Loss: 0.479466\n",
      "Epoch: 7/8... Step: 21000... Loss: 0.996043... Val Loss: 0.477992\n",
      "Epoch: 7/8... Step: 21200... Loss: 0.656113... Val Loss: 0.474762\n",
      "Validation loss decreased (0.476025 --> 0.474762).  Saving model ...\n",
      "Epoch: 7/8... Step: 21400... Loss: 0.744734... Val Loss: 0.472714\n",
      "Validation loss decreased (0.474762 --> 0.472714).  Saving model ...\n",
      "learning_rate decreased to: 0.00046948360430511846\n",
      "Epoch: 7/8... Step: 21600... Loss: 0.715982... Val Loss: 0.477432\n",
      "Epoch: 7/8... Step: 21800... Loss: 1.048011... Val Loss: 0.487462\n",
      "Epoch: 7/8... Step: 22000... Loss: 0.505477... Val Loss: 0.474339\n",
      "Epoch: 7/8... Step: 22200... Loss: 0.750628... Val Loss: 0.478353\n",
      "Epoch: 7/8... Step: 22400... Loss: 0.785403... Val Loss: 0.478262\n",
      "Epoch: 7/8... Step: 22600... Loss: 1.202571... Val Loss: 0.477157\n",
      "Epoch: 7/8... Step: 22800... Loss: 0.733038... Val Loss: 0.488646\n",
      "Epoch: 7/8... Step: 23000... Loss: 1.012085... Val Loss: 0.483131\n",
      "Epoch: 7/8... Step: 23200... Loss: 1.171651... Val Loss: 0.473632\n",
      "learning_rate decreased to: 0.0003638497933364668\n",
      "Epoch: 7/8... Step: 23400... Loss: 0.663018... Val Loss: 0.473136\n",
      "Epoch: 8/8... Step: 23600... Loss: 0.837757... Val Loss: 0.468940\n",
      "Validation loss decreased (0.472714 --> 0.468940).  Saving model ...\n",
      "Epoch: 8/8... Step: 23800... Loss: 0.731043... Val Loss: 0.477797\n",
      "Epoch: 8/8... Step: 24000... Loss: 0.754113... Val Loss: 0.481846\n",
      "Epoch: 8/8... Step: 24200... Loss: 1.028258... Val Loss: 0.494705\n",
      "Epoch: 8/8... Step: 24400... Loss: 0.872669... Val Loss: 0.470186\n",
      "Epoch: 8/8... Step: 24600... Loss: 1.063054... Val Loss: 0.474171\n",
      "Epoch: 8/8... Step: 24800... Loss: 0.652157... Val Loss: 0.466275\n",
      "Validation loss decreased (0.468940 --> 0.466275).  Saving model ...\n",
      "Epoch: 8/8... Step: 25000... Loss: 1.031947... Val Loss: 0.467910\n",
      "learning_rate decreased to: 0.00028198358983576175\n",
      "Epoch: 8/8... Step: 25200... Loss: 0.918366... Val Loss: 0.470578\n",
      "Epoch: 8/8... Step: 25400... Loss: 1.199567... Val Loss: 0.470272\n",
      "Epoch: 8/8... Step: 25600... Loss: 0.885634... Val Loss: 0.476153\n",
      "Epoch: 8/8... Step: 25800... Loss: 0.742556... Val Loss: 0.471264\n",
      "Epoch: 8/8... Step: 26000... Loss: 0.720537... Val Loss: 0.471586\n",
      "Epoch: 8/8... Step: 26200... Loss: 0.711983... Val Loss: 0.469851\n",
      "Epoch: 8/8... Step: 26400... Loss: 0.836303... Val Loss: 0.473102\n",
      "Epoch: 8/8... Step: 26600... Loss: 0.733182... Val Loss: 0.467632\n"
     ]
    }
   ],
   "source": [
    "Crypto_code = \"ADA\"\n",
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "374fdc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001B7C99E66D0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch1d2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crypto_code = \"BCH\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9de49e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.01\n",
      "Epoch: 1/8... Step: 200... Loss: 0.716425... Val Loss: 0.614430\n",
      "Validation loss decreased (inf --> 0.614430).  Saving model ...\n",
      "Epoch: 1/8... Step: 400... Loss: 1.112655... Val Loss: 0.607076\n",
      "Validation loss decreased (0.614430 --> 0.607076).  Saving model ...\n",
      "Epoch: 1/8... Step: 600... Loss: 1.189215... Val Loss: 0.598043\n",
      "Validation loss decreased (0.607076 --> 0.598043).  Saving model ...\n",
      "Epoch: 1/8... Step: 800... Loss: 1.553865... Val Loss: 0.610326\n",
      "Epoch: 1/8... Step: 1000... Loss: 1.204416... Val Loss: 0.560056\n",
      "Validation loss decreased (0.598043 --> 0.560056).  Saving model ...\n",
      "Epoch: 1/8... Step: 1200... Loss: 0.849272... Val Loss: 0.815143\n",
      "Epoch: 1/8... Step: 1400... Loss: 1.148699... Val Loss: 0.522267\n",
      "Validation loss decreased (0.560056 --> 0.522267).  Saving model ...\n",
      "Epoch: 1/8... Step: 1600... Loss: 0.989617... Val Loss: 0.528071\n",
      "learning_rate decreased to: 0.007750000000000001\n",
      "Epoch: 1/8... Step: 1800... Loss: 1.063417... Val Loss: 0.501296\n",
      "Validation loss decreased (0.522267 --> 0.501296).  Saving model ...\n",
      "Epoch: 1/8... Step: 2000... Loss: 1.191530... Val Loss: 0.512806\n",
      "Epoch: 1/8... Step: 2200... Loss: 0.803575... Val Loss: 0.522256\n",
      "Epoch: 1/8... Step: 2400... Loss: 0.813670... Val Loss: 0.567212\n",
      "Epoch: 2/8... Step: 2600... Loss: 0.973601... Val Loss: 0.616451\n",
      "Epoch: 2/8... Step: 2800... Loss: 1.079632... Val Loss: 0.516317\n",
      "Epoch: 2/8... Step: 3000... Loss: 0.783283... Val Loss: 0.525359\n",
      "Epoch: 2/8... Step: 3200... Loss: 1.284790... Val Loss: 0.587005\n",
      "Epoch: 2/8... Step: 3400... Loss: 0.509961... Val Loss: 0.474679\n",
      "Validation loss decreased (0.501296 --> 0.474679).  Saving model ...\n",
      "learning_rate decreased to: 0.006006250000000001\n",
      "Epoch: 2/8... Step: 3600... Loss: 0.887225... Val Loss: 0.541769\n",
      "Epoch: 2/8... Step: 3800... Loss: 0.871231... Val Loss: 0.467207\n",
      "Validation loss decreased (0.474679 --> 0.467207).  Saving model ...\n",
      "Epoch: 2/8... Step: 4000... Loss: 0.685007... Val Loss: 0.470364\n",
      "Epoch: 2/8... Step: 4200... Loss: 0.914001... Val Loss: 0.466268\n",
      "Validation loss decreased (0.467207 --> 0.466268).  Saving model ...\n",
      "Epoch: 2/8... Step: 4400... Loss: 0.657739... Val Loss: 0.499692\n",
      "Epoch: 2/8... Step: 4600... Loss: 0.835264... Val Loss: 0.483992\n",
      "Epoch: 2/8... Step: 4800... Loss: 0.722181... Val Loss: 0.604448\n",
      "Epoch: 2/8... Step: 5000... Loss: 0.787138... Val Loss: 0.495296\n",
      "Epoch: 3/8... Step: 5200... Loss: 1.116229... Val Loss: 0.497384\n",
      "learning_rate decreased to: 0.004654843750000001\n",
      "Epoch: 3/8... Step: 5400... Loss: 0.864068... Val Loss: 0.476723\n",
      "Epoch: 3/8... Step: 5600... Loss: 0.742156... Val Loss: 0.468765\n",
      "Epoch: 3/8... Step: 5800... Loss: 1.261621... Val Loss: 0.457609\n",
      "Validation loss decreased (0.466268 --> 0.457609).  Saving model ...\n",
      "Epoch: 3/8... Step: 6000... Loss: 0.927941... Val Loss: 0.496849\n",
      "Epoch: 3/8... Step: 6200... Loss: 0.836492... Val Loss: 0.457262\n",
      "Validation loss decreased (0.457609 --> 0.457262).  Saving model ...\n",
      "Epoch: 3/8... Step: 6400... Loss: 1.182952... Val Loss: 0.461618\n",
      "Epoch: 3/8... Step: 6600... Loss: 0.806067... Val Loss: 0.469361\n",
      "Epoch: 3/8... Step: 6800... Loss: 1.158120... Val Loss: 0.465072\n",
      "Epoch: 3/8... Step: 7000... Loss: 0.898769... Val Loss: 0.452501\n",
      "Validation loss decreased (0.457262 --> 0.452501).  Saving model ...\n",
      "learning_rate decreased to: 0.003607503906250001\n",
      "Epoch: 3/8... Step: 7200... Loss: 0.481678... Val Loss: 0.454541\n",
      "Epoch: 3/8... Step: 7400... Loss: 0.954164... Val Loss: 0.454685\n",
      "Epoch: 3/8... Step: 7600... Loss: 1.309702... Val Loss: 0.461151\n",
      "Epoch: 4/8... Step: 7800... Loss: 1.091553... Val Loss: 0.516349\n",
      "Epoch: 4/8... Step: 8000... Loss: 0.635253... Val Loss: 0.475766\n",
      "Epoch: 4/8... Step: 8200... Loss: 0.814262... Val Loss: 0.459079\n",
      "Epoch: 4/8... Step: 8400... Loss: 0.886373... Val Loss: 0.468497\n",
      "Epoch: 4/8... Step: 8600... Loss: 0.852884... Val Loss: 0.512132\n",
      "Epoch: 4/8... Step: 8800... Loss: 0.587051... Val Loss: 0.454436\n",
      "learning_rate decreased to: 0.002795815527343751\n",
      "Epoch: 4/8... Step: 9000... Loss: 0.913392... Val Loss: 0.483751\n",
      "Epoch: 4/8... Step: 9200... Loss: 1.002333... Val Loss: 0.449434\n",
      "Validation loss decreased (0.452501 --> 0.449434).  Saving model ...\n",
      "Epoch: 4/8... Step: 9400... Loss: 0.823564... Val Loss: 0.455573\n",
      "Epoch: 4/8... Step: 9600... Loss: 0.853135... Val Loss: 0.455431\n",
      "Epoch: 4/8... Step: 9800... Loss: 0.894059... Val Loss: 0.453352\n",
      "Epoch: 4/8... Step: 10000... Loss: 0.631569... Val Loss: 0.452211\n",
      "Epoch: 4/8... Step: 10200... Loss: 1.070542... Val Loss: 0.473814\n",
      "Epoch: 5/8... Step: 10400... Loss: 0.508529... Val Loss: 0.454959\n",
      "Epoch: 5/8... Step: 10600... Loss: 0.665955... Val Loss: 0.469402\n",
      "learning_rate decreased to: 0.002166757033691407\n",
      "Epoch: 5/8... Step: 10800... Loss: 1.010460... Val Loss: 0.460672\n",
      "Epoch: 5/8... Step: 11000... Loss: 0.719930... Val Loss: 0.481249\n",
      "Epoch: 5/8... Step: 11200... Loss: 0.763025... Val Loss: 0.452407\n",
      "Epoch: 5/8... Step: 11400... Loss: 0.848763... Val Loss: 0.449473\n",
      "Epoch: 5/8... Step: 11600... Loss: 0.950208... Val Loss: 0.455513\n",
      "Epoch: 5/8... Step: 11800... Loss: 1.002866... Val Loss: 0.459205\n",
      "Epoch: 5/8... Step: 12000... Loss: 0.724264... Val Loss: 0.455254\n",
      "Epoch: 5/8... Step: 12200... Loss: 0.824496... Val Loss: 0.457067\n",
      "Epoch: 5/8... Step: 12400... Loss: 0.673586... Val Loss: 0.451622\n",
      "learning_rate decreased to: 0.0016792367011108404\n",
      "Epoch: 5/8... Step: 12600... Loss: 0.825716... Val Loss: 0.448585\n",
      "Validation loss decreased (0.449434 --> 0.448585).  Saving model ...\n",
      "Epoch: 5/8... Step: 12800... Loss: 0.810083... Val Loss: 0.451319\n",
      "Epoch: 6/8... Step: 13000... Loss: 0.889045... Val Loss: 0.452016\n",
      "Epoch: 6/8... Step: 13200... Loss: 0.595284... Val Loss: 0.452950\n",
      "Epoch: 6/8... Step: 13400... Loss: 0.674490... Val Loss: 0.466742\n",
      "Epoch: 6/8... Step: 13600... Loss: 1.091270... Val Loss: 0.453955\n",
      "Epoch: 6/8... Step: 13800... Loss: 0.993578... Val Loss: 0.450357\n",
      "Epoch: 6/8... Step: 14000... Loss: 0.674157... Val Loss: 0.453044\n",
      "Epoch: 6/8... Step: 14200... Loss: 0.928378... Val Loss: 0.477594\n",
      "learning_rate decreased to: 0.0013014084433609014\n",
      "Epoch: 6/8... Step: 14400... Loss: 1.010060... Val Loss: 0.457807\n",
      "Epoch: 6/8... Step: 14600... Loss: 0.790572... Val Loss: 0.448457\n",
      "Validation loss decreased (0.448585 --> 0.448457).  Saving model ...\n",
      "Epoch: 6/8... Step: 14800... Loss: 0.983089... Val Loss: 0.447346\n",
      "Validation loss decreased (0.448457 --> 0.447346).  Saving model ...\n",
      "Epoch: 6/8... Step: 15000... Loss: 0.947062... Val Loss: 0.479782\n",
      "Epoch: 6/8... Step: 15200... Loss: 0.614388... Val Loss: 0.464352\n",
      "Epoch: 6/8... Step: 15400... Loss: 0.924598... Val Loss: 0.453536\n",
      "Epoch: 7/8... Step: 15600... Loss: 0.792406... Val Loss: 0.449796\n",
      "Epoch: 7/8... Step: 15800... Loss: 0.936438... Val Loss: 0.446650\n",
      "Validation loss decreased (0.447346 --> 0.446650).  Saving model ...\n",
      "Epoch: 7/8... Step: 16000... Loss: 0.636658... Val Loss: 0.463400\n",
      "learning_rate decreased to: 0.0010085915436046987\n",
      "Epoch: 7/8... Step: 16200... Loss: 0.838696... Val Loss: 0.449838\n",
      "Epoch: 7/8... Step: 16400... Loss: 0.662533... Val Loss: 0.449326\n",
      "Epoch: 7/8... Step: 16600... Loss: 0.393332... Val Loss: 0.446987\n",
      "Epoch: 7/8... Step: 16800... Loss: 0.530541... Val Loss: 0.446728\n",
      "Epoch: 7/8... Step: 17000... Loss: 0.950984... Val Loss: 0.448266\n",
      "Epoch: 7/8... Step: 17200... Loss: 0.505811... Val Loss: 0.447322\n",
      "Epoch: 7/8... Step: 17400... Loss: 0.932109... Val Loss: 0.446900\n",
      "Epoch: 7/8... Step: 17600... Loss: 0.649203... Val Loss: 0.447269\n",
      "Epoch: 7/8... Step: 17800... Loss: 1.005057... Val Loss: 0.450920\n",
      "learning_rate decreased to: 0.0007816584462936415\n",
      "Epoch: 7/8... Step: 18000... Loss: 1.113030... Val Loss: 0.451967\n",
      "Epoch: 8/8... Step: 18200... Loss: 1.131593... Val Loss: 0.446432\n",
      "Validation loss decreased (0.446650 --> 0.446432).  Saving model ...\n",
      "Epoch: 8/8... Step: 18400... Loss: 0.693703... Val Loss: 0.451400\n",
      "Epoch: 8/8... Step: 18600... Loss: 0.907306... Val Loss: 0.448740\n",
      "Epoch: 8/8... Step: 18800... Loss: 0.979275... Val Loss: 0.451238\n",
      "Epoch: 8/8... Step: 19000... Loss: 1.012393... Val Loss: 0.446495\n",
      "Epoch: 8/8... Step: 19200... Loss: 0.634748... Val Loss: 0.449984\n",
      "Epoch: 8/8... Step: 19400... Loss: 0.763986... Val Loss: 0.446698\n",
      "Epoch: 8/8... Step: 19600... Loss: 0.704333... Val Loss: 0.465001\n",
      "learning_rate decreased to: 0.0006057852958775722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/8... Step: 19800... Loss: 0.994365... Val Loss: 0.450611\n",
      "Epoch: 8/8... Step: 20000... Loss: 0.600391... Val Loss: 0.445156\n",
      "Validation loss decreased (0.446432 --> 0.445156).  Saving model ...\n",
      "Epoch: 8/8... Step: 20200... Loss: 0.544400... Val Loss: 0.448544\n",
      "Epoch: 8/8... Step: 20400... Loss: 0.748593... Val Loss: 0.448201\n",
      "Epoch: 8/8... Step: 20600... Loss: 0.989294... Val Loss: 0.446980\n"
     ]
    }
   ],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e50aebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001B7C958D5B0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (batch1d2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch1d3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crypto_code = \"BNB\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd585e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.01\n",
      "Epoch: 1/8... Step: 200... Loss: 0.669767... Val Loss: 0.655472\n",
      "Validation loss decreased (inf --> 0.655472).  Saving model ...\n",
      "Epoch: 1/8... Step: 400... Loss: 0.776793... Val Loss: 0.620723\n",
      "Validation loss decreased (0.655472 --> 0.620723).  Saving model ...\n",
      "Epoch: 1/8... Step: 600... Loss: 1.102229... Val Loss: 0.579936\n",
      "Validation loss decreased (0.620723 --> 0.579936).  Saving model ...\n",
      "Epoch: 1/8... Step: 800... Loss: 1.318161... Val Loss: 0.664424\n",
      "Epoch: 1/8... Step: 1000... Loss: 0.847560... Val Loss: 0.753537\n",
      "Epoch: 1/8... Step: 1200... Loss: 1.137098... Val Loss: 0.518982\n",
      "Validation loss decreased (0.579936 --> 0.518982).  Saving model ...\n",
      "Epoch: 1/8... Step: 1400... Loss: 0.976274... Val Loss: 0.501454\n",
      "Validation loss decreased (0.518982 --> 0.501454).  Saving model ...\n",
      "Epoch: 1/8... Step: 1600... Loss: 0.879374... Val Loss: 0.493177\n",
      "Validation loss decreased (0.501454 --> 0.493177).  Saving model ...\n",
      "learning_rate decreased to: 0.007750000000000001\n",
      "Epoch: 1/8... Step: 1800... Loss: 0.820422... Val Loss: 0.518545\n",
      "Epoch: 1/8... Step: 2000... Loss: 0.487363... Val Loss: 0.692268\n",
      "Epoch: 1/8... Step: 2200... Loss: 1.091472... Val Loss: 0.558832\n",
      "Epoch: 1/8... Step: 2400... Loss: 0.850870... Val Loss: 0.517523\n",
      "Epoch: 2/8... Step: 2600... Loss: 0.968045... Val Loss: 0.499732\n",
      "Epoch: 2/8... Step: 2800... Loss: 0.880392... Val Loss: 0.468598\n",
      "Validation loss decreased (0.493177 --> 0.468598).  Saving model ...\n",
      "Epoch: 2/8... Step: 3000... Loss: 0.685796... Val Loss: 0.474486\n",
      "Epoch: 2/8... Step: 3200... Loss: 0.916251... Val Loss: 0.498161\n",
      "Epoch: 2/8... Step: 3400... Loss: 1.194993... Val Loss: 0.450412\n",
      "Validation loss decreased (0.468598 --> 0.450412).  Saving model ...\n",
      "learning_rate decreased to: 0.006006250000000001\n",
      "Epoch: 2/8... Step: 3600... Loss: 1.075143... Val Loss: 0.458596\n",
      "Epoch: 2/8... Step: 3800... Loss: 0.700695... Val Loss: 0.713320\n",
      "Epoch: 2/8... Step: 4000... Loss: 0.634254... Val Loss: 0.452613\n",
      "Epoch: 2/8... Step: 4200... Loss: 0.844003... Val Loss: 0.501892\n",
      "Epoch: 2/8... Step: 4400... Loss: 0.672459... Val Loss: 0.458800\n",
      "Epoch: 2/8... Step: 4600... Loss: 0.756661... Val Loss: 0.432944\n",
      "Validation loss decreased (0.450412 --> 0.432944).  Saving model ...\n",
      "Epoch: 2/8... Step: 4800... Loss: 0.721672... Val Loss: 0.431398\n",
      "Validation loss decreased (0.432944 --> 0.431398).  Saving model ...\n",
      "Epoch: 2/8... Step: 5000... Loss: 0.726647... Val Loss: 0.490732\n",
      "Epoch: 3/8... Step: 5200... Loss: 0.812603... Val Loss: 0.441592\n",
      "learning_rate decreased to: 0.004654843750000001\n",
      "Epoch: 3/8... Step: 5400... Loss: 0.710274... Val Loss: 0.430923\n",
      "Validation loss decreased (0.431398 --> 0.430923).  Saving model ...\n",
      "Epoch: 3/8... Step: 5600... Loss: 0.879858... Val Loss: 0.438357\n",
      "Epoch: 3/8... Step: 5800... Loss: 0.794878... Val Loss: 0.429071\n",
      "Validation loss decreased (0.430923 --> 0.429071).  Saving model ...\n",
      "Epoch: 3/8... Step: 6000... Loss: 0.976047... Val Loss: 0.442093\n",
      "Epoch: 3/8... Step: 6200... Loss: 1.061926... Val Loss: 0.500474\n",
      "Epoch: 3/8... Step: 6400... Loss: 1.152804... Val Loss: 0.430826\n",
      "Epoch: 3/8... Step: 6600... Loss: 0.789715... Val Loss: 0.434588\n",
      "Epoch: 3/8... Step: 6800... Loss: 0.780555... Val Loss: 0.439832\n",
      "Epoch: 3/8... Step: 7000... Loss: 0.914044... Val Loss: 0.485544\n",
      "learning_rate decreased to: 0.003607503906250001\n",
      "Epoch: 3/8... Step: 7200... Loss: 1.055253... Val Loss: 0.452615\n",
      "Epoch: 3/8... Step: 7400... Loss: 0.809045... Val Loss: 0.516755\n",
      "Epoch: 3/8... Step: 7600... Loss: 1.013796... Val Loss: 0.465015\n",
      "Epoch: 4/8... Step: 7800... Loss: 0.705581... Val Loss: 0.431570\n",
      "Epoch: 4/8... Step: 8000... Loss: 0.730316... Val Loss: 0.425870\n",
      "Validation loss decreased (0.429071 --> 0.425870).  Saving model ...\n",
      "Epoch: 4/8... Step: 8200... Loss: 1.012408... Val Loss: 0.451904\n",
      "Epoch: 4/8... Step: 8400... Loss: 0.715486... Val Loss: 0.427295\n",
      "Epoch: 4/8... Step: 8600... Loss: 0.824137... Val Loss: 0.465081\n",
      "Epoch: 4/8... Step: 8800... Loss: 0.759626... Val Loss: 0.423045\n",
      "Validation loss decreased (0.425870 --> 0.423045).  Saving model ...\n",
      "learning_rate decreased to: 0.002795815527343751\n",
      "Epoch: 4/8... Step: 9000... Loss: 0.770717... Val Loss: 0.433105\n",
      "Epoch: 4/8... Step: 9200... Loss: 0.976223... Val Loss: 0.415564\n",
      "Validation loss decreased (0.423045 --> 0.415564).  Saving model ...\n",
      "Epoch: 4/8... Step: 9400... Loss: 0.727009... Val Loss: 0.443792\n",
      "Epoch: 4/8... Step: 9600... Loss: 0.685546... Val Loss: 0.414763\n",
      "Validation loss decreased (0.415564 --> 0.414763).  Saving model ...\n",
      "Epoch: 4/8... Step: 9800... Loss: 0.857550... Val Loss: 0.418420\n",
      "Epoch: 4/8... Step: 10000... Loss: 0.660203... Val Loss: 0.410240\n",
      "Validation loss decreased (0.414763 --> 0.410240).  Saving model ...\n",
      "Epoch: 4/8... Step: 10200... Loss: 0.880563... Val Loss: 0.411692\n",
      "Epoch: 5/8... Step: 10400... Loss: 0.909218... Val Loss: 0.410176\n",
      "Validation loss decreased (0.410240 --> 0.410176).  Saving model ...\n",
      "Epoch: 5/8... Step: 10600... Loss: 0.929738... Val Loss: 0.410503\n",
      "learning_rate decreased to: 0.002166757033691407\n",
      "Epoch: 5/8... Step: 10800... Loss: 0.820084... Val Loss: 0.409645\n",
      "Validation loss decreased (0.410176 --> 0.409645).  Saving model ...\n",
      "Epoch: 5/8... Step: 11000... Loss: 0.864057... Val Loss: 0.406714\n",
      "Validation loss decreased (0.409645 --> 0.406714).  Saving model ...\n",
      "Epoch: 5/8... Step: 11200... Loss: 0.708336... Val Loss: 0.404797\n",
      "Validation loss decreased (0.406714 --> 0.404797).  Saving model ...\n",
      "Epoch: 5/8... Step: 11400... Loss: 0.760693... Val Loss: 0.414088\n",
      "Epoch: 5/8... Step: 11600... Loss: 0.743034... Val Loss: 0.445375\n",
      "Epoch: 5/8... Step: 11800... Loss: 0.636690... Val Loss: 0.404870\n",
      "Epoch: 5/8... Step: 12000... Loss: 0.731842... Val Loss: 0.408102\n",
      "Epoch: 5/8... Step: 12200... Loss: 0.834810... Val Loss: 0.407861\n",
      "Epoch: 5/8... Step: 12400... Loss: 0.834723... Val Loss: 0.414975\n",
      "learning_rate decreased to: 0.0016792367011108404\n",
      "Epoch: 5/8... Step: 12600... Loss: 0.931884... Val Loss: 0.404284\n",
      "Validation loss decreased (0.404797 --> 0.404284).  Saving model ...\n",
      "Epoch: 5/8... Step: 12800... Loss: 1.093911... Val Loss: 0.404266\n",
      "Validation loss decreased (0.404284 --> 0.404266).  Saving model ...\n",
      "Epoch: 6/8... Step: 13000... Loss: 0.701750... Val Loss: 0.394843\n",
      "Validation loss decreased (0.404266 --> 0.394843).  Saving model ...\n",
      "Epoch: 6/8... Step: 13200... Loss: 0.557080... Val Loss: 0.395661\n",
      "Epoch: 6/8... Step: 13400... Loss: 0.929926... Val Loss: 0.403699\n",
      "Epoch: 6/8... Step: 13600... Loss: 0.764182... Val Loss: 0.393409\n",
      "Validation loss decreased (0.394843 --> 0.393409).  Saving model ...\n",
      "Epoch: 6/8... Step: 13800... Loss: 0.650530... Val Loss: 0.397159\n",
      "Epoch: 6/8... Step: 14000... Loss: 0.517876... Val Loss: 0.392097\n",
      "Validation loss decreased (0.393409 --> 0.392097).  Saving model ...\n",
      "Epoch: 6/8... Step: 14200... Loss: 0.625599... Val Loss: 0.390195\n",
      "Validation loss decreased (0.392097 --> 0.390195).  Saving model ...\n",
      "learning_rate decreased to: 0.0013014084433609014\n",
      "Epoch: 6/8... Step: 14400... Loss: 0.814423... Val Loss: 0.389086\n",
      "Validation loss decreased (0.390195 --> 0.389086).  Saving model ...\n",
      "Epoch: 6/8... Step: 14600... Loss: 0.698620... Val Loss: 0.387558\n",
      "Validation loss decreased (0.389086 --> 0.387558).  Saving model ...\n",
      "Epoch: 6/8... Step: 14800... Loss: 0.873000... Val Loss: 0.389231\n",
      "Epoch: 6/8... Step: 15000... Loss: 0.860881... Val Loss: 0.385706\n",
      "Validation loss decreased (0.387558 --> 0.385706).  Saving model ...\n",
      "Epoch: 6/8... Step: 15200... Loss: 0.962971... Val Loss: 0.387949\n",
      "Epoch: 6/8... Step: 15400... Loss: 0.656815... Val Loss: 0.385596\n",
      "Validation loss decreased (0.385706 --> 0.385596).  Saving model ...\n",
      "Epoch: 7/8... Step: 15600... Loss: 0.694657... Val Loss: 0.386602\n",
      "Epoch: 7/8... Step: 15800... Loss: 0.856802... Val Loss: 0.382156\n",
      "Validation loss decreased (0.385596 --> 0.382156).  Saving model ...\n",
      "Epoch: 7/8... Step: 16000... Loss: 0.919041... Val Loss: 0.377342\n",
      "Validation loss decreased (0.382156 --> 0.377342).  Saving model ...\n",
      "learning_rate decreased to: 0.0010085915436046987\n",
      "Epoch: 7/8... Step: 16200... Loss: 0.770535... Val Loss: 0.377276\n",
      "Validation loss decreased (0.377342 --> 0.377276).  Saving model ...\n",
      "Epoch: 7/8... Step: 16400... Loss: 0.685703... Val Loss: 0.375461\n",
      "Validation loss decreased (0.377276 --> 0.375461).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/8... Step: 16600... Loss: 0.955581... Val Loss: 0.387345\n",
      "Epoch: 7/8... Step: 16800... Loss: 0.831022... Val Loss: 0.388212\n",
      "Epoch: 7/8... Step: 17000... Loss: 0.781578... Val Loss: 0.377489\n",
      "Epoch: 7/8... Step: 17200... Loss: 0.578804... Val Loss: 0.376047\n",
      "Epoch: 7/8... Step: 17400... Loss: 0.715403... Val Loss: 0.374685\n",
      "Validation loss decreased (0.375461 --> 0.374685).  Saving model ...\n",
      "Epoch: 7/8... Step: 17600... Loss: 0.798240... Val Loss: 0.374467\n",
      "Validation loss decreased (0.374685 --> 0.374467).  Saving model ...\n",
      "Epoch: 7/8... Step: 17800... Loss: 0.698832... Val Loss: 0.376729\n",
      "learning_rate decreased to: 0.0007816584462936415\n",
      "Epoch: 7/8... Step: 18000... Loss: 0.798257... Val Loss: 0.373075\n",
      "Validation loss decreased (0.374467 --> 0.373075).  Saving model ...\n",
      "Epoch: 8/8... Step: 18200... Loss: 0.715390... Val Loss: 0.369105\n",
      "Validation loss decreased (0.373075 --> 0.369105).  Saving model ...\n",
      "Epoch: 8/8... Step: 18400... Loss: 0.620352... Val Loss: 0.380769\n",
      "Epoch: 8/8... Step: 18600... Loss: 0.891664... Val Loss: 0.374594\n",
      "Epoch: 8/8... Step: 18800... Loss: 0.643278... Val Loss: 0.365273\n",
      "Validation loss decreased (0.369105 --> 0.365273).  Saving model ...\n",
      "Epoch: 8/8... Step: 19000... Loss: 0.733712... Val Loss: 0.366150\n",
      "Epoch: 8/8... Step: 19200... Loss: 0.585709... Val Loss: 0.367214\n",
      "Epoch: 8/8... Step: 19400... Loss: 0.693994... Val Loss: 0.364868\n",
      "Validation loss decreased (0.365273 --> 0.364868).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b186f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"BTC\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8507cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"ETH\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a64f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"LTC\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563dec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6781c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"NEO\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7768a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"TRX\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "Crypto_code = \"XRP\"\n",
    "train_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_train_dataset']\n",
    "val_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_val_dataset']\n",
    "test_dataset = CRYPTO_DICT[Crypto_code][\"datasets\"]['future_price_dataset']['df_test_dataset']\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "print(training_dataloader)\n",
    "\n",
    "\n",
    "# Building the network\n",
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e53a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, Crypto_code, training_dataloader, validation_dataloader, epochs= num_epoch, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "608815f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, drop_out, output_size)\n",
    "model.load_state_dict(torch.load(\"Trained-models/LSTM-model/Future_price/BNB_price_LSTM_model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c2d33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CRYPTO_DICT[\"BNB\"][\"datasets\"]['future_price_dataset']['df_test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57d45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43582572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.4999],\n",
      "        [-1.0107],\n",
      "        [-0.5108],\n",
      "        [ 0.8360],\n",
      "        [ 0.3761]], grad_fn=<SliceBackward>), tensor([[ 0.6384],\n",
      "        [-2.7959],\n",
      "        [-0.3228],\n",
      "        [ 1.2012],\n",
      "        [ 1.2950]])]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_h = model.init_hidden(batch_size, train_on_gpu)\n",
    "for inputs, targets in testing_dataloader: \n",
    "\n",
    "    # move to GPU\n",
    "    if(train_on_gpu):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    # create new hidden varaibles \n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "    # forward \n",
    "    output, val_h = model(inputs, val_h)\n",
    "\n",
    "    # calculate val_batch accuracy \n",
    "\n",
    "    print([output[:5], targets[:5]])\n",
    "    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb3852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b593e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
